[["index.html", "1 Wet-Dry Analysis", " 1 Wet-Dry Analysis See chapter 8 for plots If you want to see the functions: https://github.com/johannabosch/wetdry_analysis/tree/main/R/utils "],["clean-up.html", "2 Clean-up 2.1 Getting started 2.2 Metadata 2.3 DEG files 2.4 Cleaning DEGs 2.5 Clipping DEGS 2.6 Parallel Batch-clipping 2.7 DEG File Issues", " 2 Clean-up 2.1 Getting started 2.2 Metadata 2.2.1 Review What we need from the metadata file: track_dataStart: first date in raw trackfile (as downloaded from the tag) dep_date: day the tag was deployed on the bird dep_dateEst: my estimate of deploy date if not reported in original datasheet ret_date: date the tag was removed from the bird ret_dateEst: estimate of retrieval date if not reported in original datasheet OR estimate of the last day of reliable light data if the logger battery died while still at sea track_dataEnd: last date in raw trackfile (as downloaded from the tag) The date window identified by these variables will include uninformative data before deployment and after retrieval, track_dataEnd can be useful in identifying if/when a logger battery died at sea (track_dataEnd &lt; ret_date). When building the logic for cleaning up the DEG files, we can’t forget to include scenarios where the device stopped collecting data before the retrieval date, so ret_date will be greater than the maximum date found in the DEG file. #load the metadata md &lt;- read.csv(file.path(data.dir, &quot;MasterMetadata_LHSP_Tracking_NL_NB_NS_06Dec2024.csv&quot;)) # let&#39;s double check - every time there is an NA in retDate_comb, there should be no wet-dry file print(md %&gt;% filter(!is.na(wetdry_filename) &amp; is.na(ret_date) &amp; is.na(ret_dateEst))) # checks out ## [1] colony tagType tagMake ## [4] tagID tagProg tagProgStartUTC ## [7] band sex dep_recap ## [10] dep_handlerInits dep_lat dep_lon ## [13] dep_burrowID dep_plotID dep_date ## [16] dep_dateEst dep_yr dep_mo ## [19] dep_datetimeUTC dep_startHandle dep_time ## [22] dep_durHandle dep_brStatus dep_burrowCont ## [25] dep_mass dep_wing dep_wingType ## [28] dep_tarsus dep_bill dep_tail ## [31] dep_eggAge dep_eggWid dep_eggLen ## [34] dep_chkAge dep_notes dep_techNotes ## [37] ret_handlerInits ret_birdYN ret_tagYN ## [40] ret_date ret_dateEst ret_yr ## [43] ret_mo ret_datetimeUTC ret_time ## [46] ret_endHandle ret_durHandle ret_burrowID ## [49] ret_burrowCont ret_mass ret_massChange ## [52] ret_eggAge ret_eggWid ret_eggLen ## [55] ret_chkAge ret_spotCardYN ret_bloodCapAmt ## [58] ret_feathType ret_feathNum ret_notes ## [61] ret_notesTissue ret_tagStatus ret_trackType ## [64] wetdry_filename track_filename track_dataStart ## [67] track_dataEnd IPorig_track_filename sourceFile ## [70] gtDep_site gtDep_lat gtDep_lon ## [73] gtDep_startDate gtDep_startTime gtDep_endDate ## [76] gtDep_endTime gtDep_startDatetimeUTC gtDep_endDatetimeUTC ## [79] gtDep_notes gtRet_site gtRet_lat ## [82] gtRet_lon gtRet_startDate gtRet_startTime ## [85] gtRet_endDate gtRet_endTime gtRet_startDatetimeUTC ## [88] gtRet_endDatetimeUTC gtRet_notes gtRet_filename ## [91] ESRF ret_brStatus simulMateTrack ## [94] repGLS repGPS repGLSGPS ## &lt;0 rows&gt; (or 0-length row.names) #double check that Migrate Tech are the only devices with wetdry data print(md %&gt;% group_by(tagMake) %&gt;% summarise(filename_count = sum(!is.na(wetdry_filename)))) ## # A tibble: 12 × 2 ## tagMake filename_count ## &lt;chr&gt; &lt;int&gt; ## 1 Biotrack: MK5040 0 ## 2 Biotrack: MK5440 0 ## 3 Biotrack: MK5540 0 ## 4 Biotrack: MK5540 C 0 ## 5 Biotrack: MK5740 0 ## 6 Biotrack: MK5740? 0 ## 7 Migrate Technology: W30A9-SEA 200 ## 8 Migrate Technology: W30A9-SEA-COOL 98 ## 9 Migrate Technology: W30A9-SEA-NOT 0 ## 10 Pathtrack: nanoFix GEO-Mini 0 ## 11 Pathtrack: nanoFix miniR3_12 0 ## 12 &lt;NA&gt; 0 #how many Migrate Tech (MT) devices total? MT_devices &lt;- md %&gt;% filter(tagMake %in% c(&quot;Migrate Technology: W30A9-SEA&quot;, &quot;Migrate Technology: W30A9-SEA-COOL&quot;)) #how many MT devices with missing wetdry files? MT_devices_missing &lt;- (MT_devices %&gt;% filter(tagMake %in% c(&quot;Migrate Technology: W30A9-SEA&quot;, &quot;Migrate Technology: W30A9-SEA-COOL&quot;), is.na(wetdry_filename)) %&gt;% select(wetdry_filename, tagID)) #how many MT devices have wetdry-data? MT_devices_retrieved &lt;- (MT_devices %&gt;% filter(tagMake %in% c(&quot;Migrate Technology: W30A9-SEA&quot;, &quot;Migrate Technology: W30A9-SEA-COOL&quot;), !is.na(wetdry_filename)) %&gt;% select(wetdry_filename, tagID)) 2.2.2 Housekeeping To clean up the metadata for clipping, we need to: * filter out the Migrate Technology devices (they are the only devices with wetdry data) * remove any rows that have NA for wetdry_filename * merge cols for estimates and non-est for deployment and return dates, #clean up the metadata so that we only include the necessary columns for clipping clipping_md &lt;- md %&gt;% filter( #filter for MT devices tagMake %in% c(&quot;Migrate Technology: W30A9-SEA&quot;, &quot;Migrate Technology: W30A9-SEA-COOL&quot;), #remove any NA rows in wetdry_filename !is.na(wetdry_filename)) %&gt;% # merge estimated and observed columns mutate( depDate_comb = as.Date(ifelse(!is.na(dep_dateEst), dep_dateEst, dep_date), format = &quot;%Y-%m-%d&quot;), retDate_comb = as.Date( ifelse(!is.na(ret_dateEst), substr(ret_dateEst, nchar(ret_dateEst) - 9, nchar(ret_dateEst)), ret_date), format = &quot;%Y-%m-%d&quot;)) %&gt;% mutate(depDate_comb = format(depDate_comb, &quot;%d/%m/%Y&quot;), #keep dates in this format to match files retDate_comb = format(retDate_comb, &quot;%d/%m/%Y&quot;)) %&gt;% #only keep necessary columns select(wetdry_filename, depDate_comb, retDate_comb, colony) # checked that number of observations in `MT_devices_retrieved` is equal to `clipping_md` Make a similar metrics_md file that we can use later on for HMM #clean up the metadata so that we only include the necessary columns for clipping metrics_md &lt;- md %&gt;% filter( #filter for MT devices tagMake %in% c(&quot;Migrate Technology: W30A9-SEA&quot;, &quot;Migrate Technology: W30A9-SEA-COOL&quot;), #remove any NA rows in wetdry_filename !is.na(wetdry_filename)) %&gt;% # merge estimated and observed columns mutate( depDate_comb = as.Date(ifelse(!is.na(dep_dateEst), dep_dateEst, dep_date), format = &quot;%Y-%m-%d&quot;), retDate_comb = as.Date( ifelse(!is.na(ret_dateEst), substr(ret_dateEst, nchar(ret_dateEst) - 9, nchar(ret_dateEst)), ret_date), format = &quot;%Y-%m-%d&quot;)) %&gt;% mutate(depDate_comb = as.Date(depDate_comb), #leave dates in standard format for when DEGS match retDate_comb = as.Date(retDate_comb)) %&gt;% mutate(bird_id = str_extract(basename(wetdry_filename), &quot;[A-Z-0-9]{5}&quot;)) %&gt;% #only keep necessary columns select(bird_id, colony, wetdry_filename, depDate_comb, retDate_comb) %&gt;% #categorize deployment periods in the dtaaframe mutate( deployment_period = case_when( depDate_comb &gt;= as.Date(&quot;2017-01-01&quot;) &amp; retDate_comb &lt;= as.Date(&quot;2018-12-31&quot;) ~ &quot;2017-2018&quot;, depDate_comb &gt;= as.Date(&quot;2018-01-01&quot;) &amp; retDate_comb &lt;= as.Date(&quot;2019-12-31&quot;) ~ &quot;2018-2019&quot;, depDate_comb &gt;= as.Date(&quot;2019-01-01&quot;) &amp; retDate_comb &lt;= as.Date(&quot;2020-12-31&quot;) ~ &quot;2019-2020&quot;, depDate_comb &gt;= as.Date(&quot;2020-01-01&quot;) &amp; retDate_comb &lt;= as.Date(&quot;2021-12-31&quot;) ~ &quot;2020-2021&quot;, depDate_comb &gt;= as.Date(&quot;2021-01-01&quot;) &amp; retDate_comb &lt;= as.Date(&quot;2022-12-31&quot;) ~ &quot;2021-2022&quot;, depDate_comb &gt;= as.Date(&quot;2022-01-01&quot;) &amp; retDate_comb &lt;= as.Date(&quot;2023-12-31&quot;) ~ &quot;2022-2023&quot;, depDate_comb &gt;= as.Date(&quot;2023-01-01&quot;) &amp; retDate_comb &lt;= as.Date(&quot;2024-12-31&quot;) ~ &quot;2023-2024&quot;, TRUE ~ &quot;Other&quot; # any records outside of these ranges )) %&gt;% filter(deployment_period != &quot;2023-2024&quot;) and then a tibble to review how many files we have per deployment period per colony deployment_summary &lt;- metrics_md %&gt;% group_by(deployment_period, colony) %&gt;% summarise(file_count=n(), .groups = &quot;drop&quot;) %&gt;% pivot_wider(names_from = colony, #make tibble with colony columns values_from = file_count, values_fill = 0 # fill missing vals with 0 ) %&gt;% mutate(TOTAL = rowSums(select(., -deployment_period))) %&gt;% bind_rows(summarise(., across(where(is.numeric), sum), deployment_period = &quot;TOTAL&quot;)) deployment_summary ## # A tibble: 7 × 8 ## deployment_period `Baccalieu - Ned Walsh` `Bon Portage` Country Gull Kent ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2017-2018 6 0 0 0 0 ## 2 2018-2019 0 5 6 10 10 ## 3 2019-2020 10 17 17 23 16 ## 4 2021-2022 20 0 16 16 27 ## 5 2022-2023 7 13 0 11 0 ## 6 Other 0 0 0 2 0 ## 7 TOTAL 43 35 39 62 53 ## # ℹ 2 more variables: `Middle Lawn` &lt;int&gt;, TOTAL &lt;dbl&gt; Looks like there were 3 devices that stayed out for longer than a year. Lets handle these later on* print(metrics_md[metrics_md$deployment_period ==&quot;Other&quot;, ]) ## bird_id colony wetdry_filename depDate_comb ## 142 BU971 Gull BU971_25Nov23_194250.deg 2019-09-22 ## 165 CD493 Gull CD493_22Jun23_004919driftadj.deg 2021-09-16 ## 234 CD501 Middle Lawn CD501_25Nov23_184453driftadj.deg 2021-09-22 ## retDate_comb deployment_period ## 142 2023-08-22 Other ## 165 2023-01-05 Other ## 234 2023-01-01 Other Save the metadata file for when we run HMM write.csv(metrics_md, file.path(data.dir, &quot;metrics_md.csv&quot;)) 2.3 DEG files The number of DEG files we have in that original folder should equal MT_devices_retrieved So we want to clip this for our known deployment times from the metadata For this bird we want to filter the DEG file to keep any dates from 2017-09-20 to 2018-06-29. #access the deg file deg_file &lt;- file.path(origin.dir, &quot;BH584_04Jul18_125355driftadj.deg&quot;) #here&#39;s what a raw deg looks like print(readLines(deg_file, n = 25)) #our data starts at line 20 ## [1] &quot;Migrate Technology Ltd logger&quot; ## [2] &quot;Type: 10.13.4, current settings: Approx 1&#39;C resolution. Conductivity &gt;63 for &#39;wets&#39; count. Light range 4, clipped OFF, max light regime. XT. &quot; ## [3] &quot;Logger number: BH584&quot; ## [4] &quot;&quot; ## [5] &quot;MODE: 6B (light, wet/dry recorded) - FOR OLD MODE 6, CLIPPED MUST BE ON&quot; ## [6] &quot;LIGHT: Sampled every minute with max light recorded every 5mins.&quot; ## [7] &quot;TEMPERATURE: NOT RECORDED.&quot; ## [8] &quot;WET/DRY: Sampled every 30secs with number of samples wet recorded every 10mins.&quot; ## [9] &quot;Max record length = 26 months. Total battery life approx 9 months. Logger is currently 11 months old.&quot; ## [10] &quot;&quot; ## [11] &quot;Programmed: 13/09/2017 17:18:06. Start of logging (DD/MM/YYYY HH:MM:SS): 13/09/2017 17:18:06&quot; ## [12] &quot;Age at start of logging (secs): 3372472, approx 1 months&quot; ## [13] &quot;End of logging (DD/MM/YYYY HH:MM:SS): 04/07/2018 12:51:58&quot; ## [14] &quot;Age at end of logging (secs): 28757996, approx 11 months&quot; ## [15] &quot;Timer (DDDDD:HH:MM:SS): 00293:19:32:04&quot; ## [16] &quot;Drift (secs): 108. Memory not full. &quot; ## [17] &quot;Pointers: 47511,105444,15000804,15000804,0,0,59% light mem used,50% wet/dry/temp mem used&quot; ## [18] &quot;Tcals (Ax^3+Bx^2+Cx+D): 12065.555,-25651.861,17238.887,-3643.479&quot; ## [19] &quot;Approx 1&#39;C resolution. Conductivity &gt;63 for &#39;wets&#39; count. Light range 4, clipped OFF, max light regime. XT. &quot; ## [20] &quot;DD/MM/YYYY HH:MM:SS\\twets0-20&quot; ## [21] &quot;13/09/2017 17:28:06\\t0&quot; ## [22] &quot;13/09/2017 17:38:06\\t0&quot; ## [23] &quot;13/09/2017 17:48:06\\t0&quot; ## [24] &quot;13/09/2017 17:58:06\\t0&quot; ## [25] &quot;13/09/2017 18:08:06\\t0&quot; Right now the DEG files contain a bunch of metadata (the first 20 lines), and data from the entire track time, from track_dataStart to track_dataEnd in the migrate_md dataframe. On line 20, you can see the two columns (datetime and wets0-20) that log data for the device. The rows after line 20 sometimes contain random errors that occur while the device is logging data, so we have clean those out too and also isolate the date from the datetime column. 2.4 Cleaning DEGs I made a clean_deg function to remove the noise from the DEG files. source(here::here(&quot;utils/clean_degs.R&quot;), echo=F) if (FALSE) { clean_degs(origin.dir, deg.dir) } Use this function to fix the retrieval dates in clipping_md when retrieval date is greater than the last day listed in the DEG. This preps metadata for clipping the DEG files based on deployment periods. source(here(&quot;utils/fix_ret_dates.R&quot;), echo = F) ## ## =================================================== ## YOU LOADED fix_ret_dates ## view this function in wey-dry_analysis/R/utils ## =================================================== ## ## this function fixes the retrieval dates in a metadata file ## In cases where retDate_comb is greater than the last date ## listed in the DEG file, the last date in the DEG file ## replaces the retrieval date. This preps the metadata for clipping the DEG files based on deployment periods. ## ## - Ensure dependencies in project_settings.R ## ============================================= #this just changes the structure of clipping_md, no files are changed fixed_md &lt;- update_ret_date(clipping_md, deg.dir) ## ## ===================================================== ## C:/Users/BoschJ/Desktop/wet-dry_analysis/data/DEG_clean/CD491_2461_13483_GLS411_NED_Walsh.deg ## ## Deployment date: 18856 - Retrieval date: 19177 ## Updating retDate_comb for row: 25 from 04/07/2022 to 10/03/2022 ## ## ===================================================== ## C:/Users/BoschJ/Desktop/wet-dry_analysis/data/DEG_clean/BU971_25Nov23_194250.deg ## ## Deployment date: 18161 - Retrieval date: 19591 ## Updating retDate_comb for row: 154 from 22/08/2023 to 12/03/2021 ## ## ===================================================== ## C:/Users/BoschJ/Desktop/wet-dry_analysis/data/DEG_clean/BP496_09Aug19_171436driftadj.deg ## ## Deployment date: 17787 - Retrieval date: 18071 ## Updating retDate_comb for row: 211 from 24/06/2019 to 22/06/2019 ## ## ===================================================== ## C:/Users/BoschJ/Desktop/wet-dry_analysis/data/DEG_clean/BU784_14Aug20_141554driftadj.deg ## ## Deployment date: 18139 - Retrieval date: 18273 ## Updating retDate_comb for row: 221 from 12/01/2020 to 06/09/2019 #looks like retrieval dates for 5 files had to be fixed When the function prints this output, the dates are numerical but I double checked and it is doing it properly so dates are just being printed kind of wonky in the output. 4 files had to be changed because the last date in the DEG file was earlier than the retrieval date: * BU784_14Aug20_141554driftadj.deg * BP496_09Aug19_171436driftadj.deg * BU971_25Nov23_194250.deg * CD491_2461_13483_GLS411_NED_Walsh.deg 2.5 Clipping DEGS I’m using Furrr to speed up the clipping task. Purrr basically replaces R’s apply family, and Furrr extends Purrr to support parallel processing. Purrr’s pmap() family applies a function row-wise over multiple inputs and bins the results into a single data frame (pmap_dfr - data frame row-bind). This works for our big clip job because we can use Furrr in combination with dplyr functions like mutate() summarize() or filter() to optimize clipping, we can still process a single file at a time to avoid overloading memory, but we can distribute the work across multiple cores for faster execution so that one core deals with 50 files I clipped the data sequentially with Purrr for 10 files first to get the logic down, and then switched to Furrr to build a bigger function (clip_degs.R). source(here(&quot;utils/clip_deg.R&quot;), echo = F) ## ## =================================================== ## YOU LOADED clip_degs ## view this function in wey-dry_analysis/R/utils ## =================================================== ## ## this function clips DEG files based on deployment periods ## for combined estimated and observed dates, where depDate_comb ## is the deployment date and retDate_comb is the retrieval date ## ## ## - Ensure dependencies in project_settings.R ## ============================================= #run on a subset of 10 files to start subset &lt;- fixed_md[1:10, ] str(subset) ## &#39;data.frame&#39;: 10 obs. of 4 variables: ## $ wetdry_filename: chr &quot;BH594_04Jul18_135310driftadj.deg&quot; &quot;BH596_04Jul18_134429driftadj.deg&quot; &quot;BH602_04Jul18_132708driftadj.deg&quot; &quot;BH584_04Jul18_125355driftadj.deg&quot; ... ## $ depDate_comb : chr &quot;20/09/2017&quot; &quot;20/09/2017&quot; &quot;20/09/2017&quot; &quot;20/09/2017&quot; ... ## $ retDate_comb : chr &quot;29/06/2018&quot; &quot;29/06/2018&quot; &quot;29/06/2018&quot; &quot;01/07/2018&quot; ... ## $ colony : chr &quot;Baccalieu - Ned Walsh&quot; &quot;Baccalieu - Ned Walsh&quot; &quot;Baccalieu - Ned Walsh&quot; &quot;Baccalieu - Ned Walsh&quot; ... # Loop through each row in the &#39;subset&#39; data frame and call clip_deg if (FALSE) { profvis({ for (i in 1:nrow(subset)) { print(paste(&quot;Processing row:&quot;, i)) print(subset[i, , drop = FALSE]) result &lt;- clip_deg(subset[i, , drop = FALSE], deg.dir, output.dir, zip_file, log_file) }}) # using write_csv() the Date column (now of type Date) is automatically # converted to the standard YYYY-MM-DD format - default behavior of the # readr package so I&#39;ll just keep it that way } Without using Furrr process took : * 85.6MB of memory on my machine * 5.27 seconds for 10 files 2.6 Parallel Batch-clipping I have 8 cores on my machine #define the number of cores on your machine cores &lt;- parallel::detectCores() 2.6.1 Testing # define function for a single row with clip deg process_row &lt;- function(row, deg.dir, output.dir, log_file) { clip_deg(row, deg.dir, output.dir, log_file) } I’m testing Furrr with 7 and then 4 cores to process 10 files to start, then scale it up to 50 files if (FALSE) { # first I&#39;ll try using half of my cores # and then try using one less than what I have available plan(multisession, workers = (cores/2)) plan() profvis({ results &lt;- future_map_dfr( .x = split(subset, seq_len(nrow(subset))), .f = ~ process_row(.x, deg.dir, output.dir, log_file), .progress = TRUE) }) print(results) } Only took up 10.9MB of memory and around 4.64 seconds to run 10 files on 7 cores On 4 cores it took 5.1MB and 3.74 seconds to run (using 4 cores) Try scaling up to 50 files with 4 cores to see how it performs if (FALSE) { subset &lt;- fixed_md[1:50, ] str(subset) plan() profvis({ results &lt;- future_map_dfr( .x = split(subset, seq_len(nrow(subset))), .f = ~ process_row(.x, deg.dir, output.dir, zip_file, log_file), .progress = TRUE) }) } With 4 cores for 50 files, Furrr::future_map_dfr used 6.4MB of memory and took 7.62 seconds. 2.6.2 Final clip Let’s run it on all files in deg.dir created a sink in clip_degs to log output to a file instead of console if (FALSE) { subset &lt;- fixed_md[1:10, ] str(subset) aggregate_logs &lt;- function(output.dir, log_file) { worker_logs &lt;- list.files(output.dir, pattern = &quot;clip_summary_worker_.*\\\\.txt&quot;, full.names = TRUE) file.append(log_file, worker_logs) file.remove(worker_logs) } log_file &lt;- file.path(output.dir, &quot;clip_summary.txt&quot;) if (file.exists(log_file)) file.remove(log_file) #restart R and clear env, then try with all files on 4 cores plan(sequential) # use sequential for testing #switch to `multisession` for parallel results &lt;- future_map_dfr( .x = split(subset, seq_len(nrow(subset))), .f = ~ clip_deg(.x, deg.dir, output.dir), .progress = TRUE ) print(results) } if (FALSE) { # Switch to parallel processing for the full dataset plan(multisession, workers = cores/2) if (file.exists(log_file)) file.remove(log_file) worker_logs &lt;- list.files(output.dir, pattern = &quot;^clip_summary_worker_.*\\\\.txt$&quot;, full.names = TRUE) if (length(worker_logs) &gt; 0) file.remove(worker_logs) # Process the full dataset results_full &lt;- future_map_dfr( .x = split(fixed_md, seq_len(nrow(fixed_md))), .f = ~ clip_deg(.x, deg.dir, output.dir), .progress = TRUE ) # Consolidate logs aggregate_logs(output.dir, log_file) print(results_full) } 274 files on 4 cores required 9.2MB of memory allocation and around 58 seconds. 2.7 DEG File Issues Make a new metadata file that reviews the days deployed deployment_md &lt;- clipping_md %&gt;% mutate( depDate_comb = as.Date(depDate_comb, format = &quot;%d/%m/%Y&quot;), retDate_comb = as.Date(retDate_comb, format = &quot;%d/%m/%Y&quot;), days_deployed = as.numeric(retDate_comb - depDate_comb) +1 ) %&gt;% rowwise() %&gt;% mutate( last_active_day = { deg_file &lt;- file.path(deg.dir, wetdry_filename) if (file.exists(deg_file)) { deg_data &lt;- tryCatch( read_csv(deg_file, col_types = cols( #define column types date = col_date(format = &quot;%d/%m/%Y&quot;), time = col_character(), `wets0-20` = col_double() )), error = function(e) NULL ) if (!is.null(deg_data)) { max(deg_data$date, na.rm = TRUE) #use the max deg date } else { NA } } else { NA } }, days_inactive_before_ret = if (retDate_comb &gt; last_active_day) { as.numeric(retDate_comb - last_active_day)+1 } else { NA }, days_active_before_ret = as.numeric(last_active_day-depDate_comb)+1, days_active_after_ret = if (last_active_day &gt; retDate_comb) { as.numeric(last_active_day - retDate_comb)+1 } else { NA }, ) %&gt;% ungroup() %&gt;% select(wetdry_filename, depDate_comb, retDate_comb, last_active_day, days_deployed, days_active_before_ret, days_inactive_before_ret, days_active_after_ret) deployment_md ## # A tibble: 298 × 8 ## wetdry_filename depDate_comb retDate_comb last_active_day days_deployed ## &lt;chr&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;dbl&gt; ## 1 BH594_04Jul18_135310… 2017-09-20 2018-06-29 2018-07-04 283 ## 2 BH596_04Jul18_134429… 2017-09-20 2018-06-29 2018-07-04 283 ## 3 BH602_04Jul18_132708… 2017-09-20 2018-06-29 2018-07-04 283 ## 4 BH584_04Jul18_125355… 2017-09-20 2018-07-01 2018-07-04 285 ## 5 BH603_03Jul18_194628… 2017-09-19 2018-07-01 2018-07-03 286 ## 6 BH595_04Jul18_131651… 2017-09-19 2018-07-01 2018-07-04 286 ## 7 BU853_69W_4261-13034… 2019-08-27 2020-10-26 2021-04-19 427 ## 8 BU869_77W_2461-13037… 2019-08-27 2020-11-06 2021-02-09 438 ## 9 BU832_24Jul20_165309… 2019-08-27 2020-07-22 2020-07-24 331 ## 10 BU847_84W_2461-13042… 2019-08-27 2020-11-05 2021-02-19 437 ## # ℹ 288 more rows ## # ℹ 3 more variables: days_active_before_ret &lt;dbl&gt;, ## # days_inactive_before_ret &lt;dbl&gt;, days_active_after_ret &lt;dbl&gt; There should only be 4 files with values for days_active_after_ret, showing how many days the device was dead before retrieval We can also see from this table how many days devices were deployed, how long some devices were on after retrieval, etc. 2.7.0.1 File: CD480_06Aug22_140316driftadj.deg Noticed file CD480_06Aug22_140316driftadj.deg has DEG some dates listed as 2201-10-06 file &lt;- deployment_md[deployment_md$wetdry_filename == &quot;CD480_06Aug22_140316driftadj.deg&quot;, ] The DEG file has a weird invalid byte error message and then date formatting messed up. On May 2022, 01/05/2022, the device starts logging invalid bytes then says it starts logging again in January of the next year? listed as 18/01/2201, It can’t be logging for 2022 anymore because the day 18/01 already passed in 2022 and metadata says retrieval day was in August 2022, 2022-07-31 So it says it started logging in January again but lists the date as 2201 ``` 01/05/2022 07:09:47 20 01/05/2022 07:19:47 20 01/05/2022 07:29:47 20 01/05/2022 07:39:47 20 01/05/2022 07:49:47 20 Invalid byte: FF Invalid byte: FF Invalid byte: FF Invalid byte: FF Invalid byte: FF 01/05/2022 07:59:47 20 01/05/2022 08:09:47 20 01/05/2022 08:19:47 20 01/05/2022 08:29:47 20 01/05/2022 08:39:47 20 ... 01/05/2022 21:49:47 20 01/05/2022 21:59:47 20 01/05/2022 22:09:47 20 01/05/2022 22:19:47 20 Missing data until 18/01/2201 08:39:17 Invalid byte: FF Invalid byte: FF Invalid byte: FF Invalid byte: FF Invalid byte: FF ... Invalid byte: FF Invalid byte: FF 18/01/2201 08:40:40 20 18/01/2201 08:50:40 20 18/01/2201 09:00:40 20 ... ``` When we applied clip_deg it filtered out all those messy dates leaving any dates before 01/05/2022, so we’re good to just keep using this file but it is bad quality. The wets values from April 30 at 8AM to May 1 at 7AM were consistently equal to 20, so maybe something went wrong with the device there (30/04/2022 08:19:47 to 01/05/2022 07:49:47). So really the last active day for this device was 01/05/2022 2.7.0.2 File: BU784_14Aug20_141554driftadj.deg Also the file BU784_14Aug20_141554driftadj.deg only has 937 lines after filtering Looks like it was deployed for 135 days and died 7 days after it was deployed file &lt;- deployment_md[deployment_md$wetdry_filename == &quot;BU784_14Aug20_141554driftadj.deg&quot;, ] print(file) ## # A tibble: 1 × 8 ## wetdry_filename depDate_comb retDate_comb last_active_day days_deployed ## &lt;chr&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;dbl&gt; ## 1 BU784_14Aug20_141554d… 2019-08-31 2020-01-12 2019-09-06 135 ## # ℹ 3 more variables: days_active_before_ret &lt;dbl&gt;, ## # days_inactive_before_ret &lt;dbl&gt;, days_active_after_ret &lt;dbl&gt; **Going to hold these cleaned up and clipped files in R’s memory instead of writing them to a new csv every time. Will remove the DEG_clean dir, but keep the final clipped DEGS in /clipped so we don’t overload R "],["wet-dry-switches.html", "3 Wet-Dry Switches 3.1 Calculating Switches", " 3 Wet-Dry Switches Within each 2hr period we need to assess: • Number of switches between 0 and non-zero value 3.1 Calculating Switches Before downsampling the data let’s calculate the number of switches between wet and dry states in a 10min interval. This will be a binary indicator (wets&gt;0 or wets==0) that we can use along with the proportions. Our current DEG files look like this: date,time,wets0-20 2017-09-20,00:08:08,0 2017-09-20,00:18:08,0 2017-09-20,00:28:08,0 2017-09-20,00:38:08,0 2017-09-20,00:48:08,0 2017-09-20,00:58:08,0 2017-09-20,01:08:08,0 To calculate switches between 10min intervals, we need to transform wet values to a binary Example // If we have a vector of wets equal to: [0, 20, 18, 0, 4, 2] The binary would be: [0, 1, 1, 0, 1, 1] Then we need to calculate the difference between each value. diff(c(0, 1, 1, 0, 1, 1)) Output: [1, 0, -1, 1, 0] Finally, we don’t want negatives, so we can count absolute values abs(c(1, 0, -1, 1, 0)) Final Output: [1, 0, 1, 1, 0] The function below does this for one file, adding a column to our DEG files that tracks switches calc_switch &lt;- function(file_path) { deg_data &lt;- read_csv(file_path, show_col_types=FALSE) bird_id &lt;- str_extract(basename(file_path), &quot;(?&lt;=filtered_)[A-Z-0-9]{5}&quot;) wets &lt;- deg_data$`wets0-20` #switches between wet (non-zero) and dry (zero) states #row_number() assigns a unique number to each row to reference row positions #if it is the first row, set state to 0 since there are no rows to compare with #for all other rows (else case), calculate the numb. of switches switches =c(0, abs(diff( #calculates absolute values for state changes (1 or -1 depending on direction) ifelse(wets &gt; 0, 1, 0)))) #if wets&gt;0 use 1, if not use 0 (transforms wets to binary) deg_data &lt;- deg_data %&gt;% mutate(bird_id = bird_id, switches = switches) %&gt;% rename(wets = `wets0-20`) %&gt;% select(bird_id, date, time, wets, switches) return(deg_data) } Then we can apply that function to all our files in our clipped directory #define deg files to use deg_files &lt;- list.files(output.dir, pattern = &quot;*.deg&quot;, full.names = TRUE)#[1:10] #add if you want to subset plan(multisession, workers = 4) deg_switches &lt;- future_map_dfr(deg_files, calc_switch, .progress = TRUE) plan(sequential) Add the colony and deployment_period values from metrics_md to deg_switches deg_switches &lt;- deg_switches %&gt;% left_join(metrics_md %&gt;% select(bird_id, deployment_period, colony), by = &quot;bird_id&quot;) %&gt;% filter(deployment_period != &quot;2023-2024&quot;) "],["longest-wetdry-periods.html", "4 Longest Wet/Dry Periods", " 4 Longest Wet/Dry Periods • Longest continuous time dry (longest number of consecutive rows with wets==0 * 10min) • Longest continuous time wet (longest number of consecutive rows with wets&gt;=1 * 10min) Here I use rle() (run length encoding) to find consecutive runs of vals above 0 or equal to 20. This produces a vector values made up of TRUE/FALSE vals, and lengths which tracks the number of TRUE/FALSE vals occurring in a row. example // vec &lt;- c(TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE) len = c(2,3,3,1) Without looking at when the periods are occurring calc_longest_prds &lt;- function(file_path) { deg_data &lt;- read_csv(file_path, show_col_types = FALSE) # define bird IDs bird_id &lt;- str_extract(basename(file_path), &quot;(?&lt;=filtered_)[A-Z-0-9]{5}&quot;) #find the longest period longest_run &lt;- function(condition) { rle_data &lt;- rle(condition) #run length encode values and lengths for condition #max run length for runs max(rle_data$lengths[rle_data$values], na.rm = TRUE)} #of dry times longest_dry_intervals &lt;- longest_run(deg_data$`wets0-20` == 0) longest_dry_time &lt;- longest_dry_intervals * 10 #convert to mins #of wet times longest_wet_intervals &lt;- longest_run(deg_data$`wets0-20` &gt;= 1) longest_wet_time &lt;- longest_wet_intervals * 10 #convert to mins #return results tibble( bird_id=bird_id, longest_dry_time = longest_dry_time, longest_wet_time = longest_wet_time) } #define deg files to use deg_files &lt;- list.files(output.dir, pattern = &quot;*.deg&quot;, full.names = TRUE) #add if you want to subset plan(multisession, workers = 4) # run on gull 2017-2018 files longest_prds &lt;- future_map_dfr(deg_files, calc_longest_prds) longest_prds ## # A tibble: 298 × 3 ## bird_id longest_dry_time longest_wet_time ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BH584 4550 1140 ## 2 BH594 5690 840 ## 3 BH595 5800 2600 ## 4 BH596 4580 920 ## 5 BH602 6060 1150 ## 6 BH603 7520 2300 ## 7 BP488 1920 1410 ## 8 BP490 4630 5800 ## 9 BP491 6040 1660 ## 10 BP492 7060 3780 ## # ℹ 288 more rows If we want to know when the periods are occurring find_longest_prds &lt;- function(file_path) { deg_data &lt;- read_csv(file_path, show_col_types = FALSE) # define bird IDs bird_id &lt;- str_extract(basename(file_path), &quot;(?&lt;=filtered_)[A-Z-0-9]{5}&quot;) deg_data &lt;- deg_data %&gt;% mutate(datetime = as.POSIXct(paste(date, time), format = &quot;%Y-%m-%d %H:%M:%S&quot;)) # Find the longest run and its start/end times longest_run &lt;- function(condition) { rle_data &lt;- rle(condition) # Run-length encode the condition max_length &lt;- max(rle_data$lengths[rle_data$values], na.rm = TRUE) # Max run length start_index &lt;- which.max(rle_data$lengths * rle_data$values) # Index of the max run start_row &lt;- cumsum(rle_data$lengths)[start_index - 1] + 1 # First row of the run end_row &lt;- start_row + max_length - 1 # Last row of the run list(length = max_length, start = start_row, end = end_row) } # Dry times dry_run &lt;- longest_run(deg_data$`wets0-20` == 0) longest_dry_time &lt;- dry_run$length * 10 # Convert to minutes longest_dry_start &lt;- deg_data$datetime[dry_run$start] longest_dry_end &lt;- deg_data$datetime[dry_run$end] # Wet times wet_run &lt;- longest_run(deg_data$`wets0-20` &gt;= 1) longest_wet_time &lt;- wet_run$length * 10 # Convert to minutes longest_wet_start &lt;- deg_data$datetime[wet_run$start] longest_wet_end &lt;- deg_data$datetime[wet_run$end] # Return results tibble( bird_id = bird_id, longest_dry_time = longest_dry_time, longest_dry_start = longest_dry_start, longest_dry_end = longest_dry_end, longest_wet_time = longest_wet_time, longest_wet_start = longest_wet_start, longest_wet_end = longest_wet_end ) } #define deg files to use deg_files &lt;- list.files(output.dir, pattern = &quot;*.deg&quot;, full.names = TRUE) #add if you want to subset plan(multisession, workers = 4) # run on all deg files deg_longest_prds_time &lt;- future_map_dfr(deg_files, find_longest_prds, .progress = TRUE) deg_longest_prds &lt;- deg_longest_prds_time %&gt;% left_join(metrics_md %&gt;% select(bird_id, deployment_period, colony), by = &quot;bird_id&quot;) %&gt;% mutate( longest_dry_start = as.POSIXct(longest_dry_start, format = &quot;%Y-%m-%d %H:%M:%S&quot;, tz = &quot;UTC&quot;), longest_dry_end = as.POSIXct(longest_dry_end, format = &quot;%Y-%m-%d %H:%M:%S&quot;, tz = &quot;UTC&quot;), longest_wet_start = as.POSIXct(longest_wet_start, format = &quot;%Y-%m-%d %H:%M:%S&quot;, tz = &quot;UTC&quot;), longest_wet_end = as.POSIXct(longest_wet_end, format = &quot;%Y-%m-%d %H:%M:%S&quot;, tz = &quot;UTC&quot;)) %&gt;% filter(deployment_period != &quot;2023-2024&quot;) "],["total-wetdry.html", "5 Total Wet/Dry", " 5 Total Wet/Dry For this we want to know: • Total time when entire 10min sampling interval was dry (count 0s * 10min) • Total time when entire 10min sampling interval was wet (count &gt;=1 * 10min) calc_dry_time &lt;- function(deg_data) { #count rows where wets == 0 total_dry_intervals &lt;- sum(deg_data$`wets0-20` == 0, na.rm = TRUE) total_time_dry &lt;- total_dry_intervals * 10 #multiply by 10mins per interval return(total_time_dry) } calc_wet_time &lt;- function(deg_data) { total_wet_intervals &lt;- sum(deg_data$`wets0-20` &gt;=1 , na.rm = TRUE) total_time_wet &lt;- total_wet_intervals * 10 return(total_time_wet) } #then we need a function to run these on multiple files calc_times &lt;- function(file_path) { deg_data &lt;- read_csv(file_path, show_col_types = FALSE) bird_id &lt;- str_extract(basename(file_path), &quot;(?&lt;=filtered_)[A-Z-0-9]{5}&quot;) total_time_dry &lt;- calc_dry_time(deg_data) total_time_wet &lt;- calc_wet_time(deg_data) tibble( bird_id = bird_id, total_time_dry = total_time_dry, total_time_wet = total_time_wet) } deg_files &lt;- list.files(output.dir, pattern = &quot;*.deg&quot;, full.names = TRUE)#[1:10] #add if you want to subset plan(multisession, workers = 4) deg_total_wetdry &lt;- future_map_dfr(deg_files, calc_times, .progress = TRUE) plan(sequential) deg_total_wetdry &lt;- deg_total_wetdry %&gt;% left_join(metrics_md %&gt;% select(bird_id, deployment_period, colony), by = &quot;bird_id&quot;) %&gt;% filter(deployment_period != &quot;2023-2024&quot;) "],["downsampling.html", "6 Downsampling 6.1 Down-sampling (MEAN wets)", " 6 Downsampling 6.1 Down-sampling (MEAN wets) **Katie&#39;s Notes:** Right now the files are logging every 10 minutes, which is too high of a resolution. Need to down-sample the dataset from 10min to some other interval that is meaningful to define states to. 12hrs seems too long given how far birds can move in this time, but 10min is far too short. Tempted to use 2hr resolution to match GPS. NOTE: wet values should be summed when down-sampling for calculating proportions, and not mean wets. 6.1.1 2hr Periods source(here::here(&quot;utils/downsample_deg.R&quot;), echo = F) ## ## =================================================== ## YOU LOADED downsample_deg ## view this function in wey-dry_analysis/R/utils ## =================================================== ## ## this function subsamples DEG files by an allotted time period ## Combines each 10 minute period into X hr periods ## (define X using bin_time), and takes the sum of ## wets0-20 for each bin_time period. ## ## ## - Ensure dependencies in project_settings.R ## - Define downsampled.dir in project_settings.R ## ============================================= bin_time = 2 #define deg files to use deg_files &lt;- list.files(output.dir, pattern = &quot;*.deg&quot;, full.names = TRUE)#[1:10] #add if you want to subset #set up a furrr plan for using 4 cores plan(multisession, workers = 4) deg_2hrs &lt;- future_map(deg_files, downsample_deg, bin_time = bin_time, .progress = TRUE) plan(sequential) wets = 0 means 10min interval was entirely dry wets = 20 means 10min interval was entirely wet For one 10min interval, if wets=1, then 600s/20 = 30 seconds so 1 unit wets = 30 seconds wet per 10min interval So our dataset’s wets values now represent the average number of 10-minute intervals that were wet during the 2-hour period. This means each 2hr period has 12 intervals of 10 minutes (600 secs). "],["proportions-wet.html", "7 Proportions Wet 7.1 Calclate Proportions", " 7 Proportions Wet Within each 2hr period we need to assess: • Proportion wet or time wet ((Count wets in 2hr period * 30sec) / 72000sec) Calculate proportions for downsampled data where the sum of wets was calculated in 2hr bins 7.1 Calclate Proportions Function to calculate proportion of wet time on the downsampled data: calc_prop &lt;- function(deg_data, bin_time = bin_time) { bird_id &lt;- deg_data$bird_id[1] #start by calculating metrics for a single 2hr bin prop_data &lt;- deg_data %&gt;% mutate( bird_id = bird_id, #proportion of time wet in bin_time period (wets*30 converts wetness level into secs spent wet) # (600sec per 10min interval) / 20 = 30 seconds per unit prop_wet = (wets * 30) / (bin_time * 3600) #convert wets to secs and divide by bin_length in secs ) return(prop_data) } Applying the function to our DEG files downsampled by 2hrs using sum of wets: plan(multisession, workers = 4) bin_time = 2 deg_props_2hrs &lt;- future_map_dfr(deg_2hrs, calc_prop, bin_time = bin_time, .progress = TRUE) deg_props_2hrs ## # A tibble: 1,066,147 × 6 ## bird_id date start_time end_time wets prop_wet ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BH584 2017-09-20 00:00:00 01:59:59 0 0 ## 2 BH584 2017-09-20 02:00:00 03:59:59 0 0 ## 3 BH584 2017-09-20 04:00:00 05:59:59 32 0.133 ## 4 BH584 2017-09-20 06:00:00 07:59:59 78 0.325 ## 5 BH584 2017-09-20 08:00:00 09:59:59 36 0.15 ## 6 BH584 2017-09-20 10:00:00 11:59:59 18 0.075 ## 7 BH584 2017-09-20 12:00:00 13:59:59 10 0.0417 ## 8 BH584 2017-09-20 14:00:00 15:59:59 62 0.258 ## 9 BH584 2017-09-20 16:00:00 17:59:59 49 0.204 ## 10 BH584 2017-09-20 18:00:00 19:59:59 21 0.0875 ## # ℹ 1,066,137 more rows Add the colony and deployment_period values from metrics_md to deg_props_2hrs deg_props_2hrs &lt;- deg_props_2hrs %&gt;% left_join(metrics_md %&gt;% select(bird_id, deployment_period, colony), by = &quot;bird_id&quot;) %&gt;% filter(deployment_period != &quot;2023-2024&quot;) "],["visualizing-metrics.html", "8 Visualizing Metrics 8.1 Baccalieu Island 2017-2018 8.2 Kent Island 2018-2019 8.3 Random subset", " 8 Visualizing Metrics From Katie&#39;s Notes: Within each 2hr period we need to assess: • Proportion wet or time wet ((Count wets in 2hr period * 30sec) / 72000sec) • Number of switches between 0 and non-zero value • Total time when entire 10min sampling interval was dry (count 0s * 10min) • Total time when entire 10min sampling interval was wet (count &gt;=1 * 10min) • Longest continuous time dry (longest number of consecutive rows with wets==0 * 10min) • Longest continuous time wet (longest number of consecutive rows with wets&gt;=1 * 10min) I start with setting up some plotting functions so we can look at different subsets of our data easily. Then I apply those plotting functions to various sets of the DEG files (6 files) to visualize the metrics we discussed (for Baccalieu, Kent’s Island, and random files). Let’s see what we have to work with per colony/deployment_periods deployment_summary ## # A tibble: 7 × 8 ## deployment_period `Baccalieu - Ned Walsh` `Bon Portage` Country Gull Kent ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2017-2018 6 0 0 0 0 ## 2 2018-2019 0 5 6 10 10 ## 3 2019-2020 10 17 17 23 16 ## 4 2021-2022 20 0 16 16 27 ## 5 2022-2023 7 13 0 11 0 ## 6 Other 0 0 0 2 0 ## 7 TOTAL 43 35 39 62 53 ## # ℹ 2 more variables: `Middle Lawn` &lt;int&gt;, TOTAL &lt;dbl&gt; 8.1 Baccalieu Island 2017-2018 Let’s start by visualizing switches/proportions for 2017-2018 files from Baccalieu (6 files total) 8.1.1 Running plotting functions Proportions: #subset the dataframe so we have less data to work with bacc_2017_props &lt;- deg_props_2hrs %&gt;% filter(colony == &quot;Baccalieu - Ned Walsh&quot;, deployment_period == &quot;2017-2018&quot;) %&gt;% select(bird_id, date, start_time, end_time, prop_wet) head(bacc_2017_props) ## # A tibble: 6 × 5 ## bird_id date start_time end_time prop_wet ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BH584 2017-09-20 00:00:00 01:59:59 0 ## 2 BH584 2017-09-20 02:00:00 03:59:59 0 ## 3 BH584 2017-09-20 04:00:00 05:59:59 0.133 ## 4 BH584 2017-09-20 06:00:00 07:59:59 0.325 ## 5 BH584 2017-09-20 08:00:00 09:59:59 0.15 ## 6 BH584 2017-09-20 10:00:00 11:59:59 0.075 plot_bacc2017_props &lt;- plot_props(data = bacc_2017_props, y_var = &quot;prop_wet&quot;, y_label = &quot;Proportion&quot;, plot_title = &quot;Proportion of Wet Time Across Deployments Per 2hr bins) - (Baccalieu Island 2017-2018)&quot;) This kind of hurts to look at so we can also try aggregating the data to days taking the sum of prop_wet. Daily Proportions: bacc_2017_daily_props &lt;- bacc_2017_props %&gt;% group_by(bird_id, date) %&gt;% summarize(daily_prop_wet = mean(prop_wet, na.rm = TRUE), .groups = &quot;drop&quot;) plot_bacc2017_daily_props &lt;- plot_daily_props(data = bacc_2017_daily_props, y_var = &quot;daily_prop_wet&quot;, y_label = &quot;Proportion&quot;, plot_title = &quot;Mean Proportion of Wet Periods (2hr bins) Per Day (Baccalieu Island 2017-2018)&quot;) Daily Switches: #subset the dataframe so we have less data to work with bacc_2017_switches &lt;- daily_switches %&gt;% filter(colony == &quot;Baccalieu - Ned Walsh&quot;, deployment_period == &quot;2017-2018&quot;) %&gt;% select(bird_id, date, daily_switches) head(bacc_2017_switches) ## # A tibble: 6 × 3 ## bird_id date daily_switches ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 BH584 2017-09-20 45 ## 2 BH584 2017-09-21 38 ## 3 BH584 2017-09-22 43 ## 4 BH584 2017-09-23 33 ## 5 BH584 2017-09-24 25 ## 6 BH584 2017-09-25 28 plot_bacc2017_daily_switches &lt;- plot_daily_switches( data = bacc_2017_switches, y_var = &quot;daily_switches&quot;, y_label = &quot;Sum of Switches&quot;, plot_title = &quot;Daily Sum of Switches between Wet (wet&gt;0) and Dry (wet=0) States for Baccalieu Island (2017-2018)&quot;) Longest Wet/Dry Periods bacc_2017_longest_prds &lt;- deg_longest_prds %&gt;% filter(colony == &quot;Baccalieu - Ned Walsh&quot;, deployment_period == &quot;2017-2018&quot;) %&gt;% select(bird_id, longest_dry_time, longest_dry_start, longest_dry_end, longest_wet_time, longest_wet_start, longest_wet_end) plot_bacc_2017_longest_prds &lt;- plot_longest_prds( data = bacc_2017_longest_prds, plot_title = &quot;Longest Wet and Dry Periods - Baccalieu Island 2017-2018&quot;) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. Total Wet/Dry bacc_2017_total_wetdry &lt;- deg_total_wetdry %&gt;% filter(colony == &quot;Baccalieu - Ned Walsh&quot;, deployment_period == &quot;2017-2018&quot;) %&gt;% select(bird_id, total_time_wet, total_time_dry) plot_bacc_2017_total_times &lt;- plot_total_times(bacc_2017_total_wetdry, &quot;Total Wet and Dry Times (Baccalieu Island, 2017-2018)&quot;) 8.1.2 Final Plots plot_bacc2017_props plot_bacc2017_daily_props plot_bacc2017_daily_switches plot_bacc_2017_total_times plot_bacc_2017_longest_prds 8.2 Kent Island 2018-2019 Let’s visualize the switches/proportions for 2018-2019 files from Kent’s Island (10 files total). Make sure you clear out your environment so you don’t accidentally run this on Baccalieu-related objects or other sites you may have tried out. rm(list = ls(pattern = &quot;^bacc&quot;)) 8.2.1 Running plotting functions Let’s start by subsetting the metadata file so we can look at Kent’s Island deployment_summary ## # A tibble: 7 × 8 ## deployment_period `Baccalieu - Ned Walsh` `Bon Portage` Country Gull Kent ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2017-2018 6 0 0 0 0 ## 2 2018-2019 0 5 6 10 10 ## 3 2019-2020 10 17 17 23 16 ## 4 2021-2022 20 0 16 16 27 ## 5 2022-2023 7 13 0 11 0 ## 6 Other 0 0 0 2 0 ## 7 TOTAL 43 35 39 62 53 ## # ℹ 2 more variables: `Middle Lawn` &lt;int&gt;, TOTAL &lt;dbl&gt; Let’s use the 10 files from 2018-2019. Proportions: #subset the dataframe so we have less data to work with kent_2018_props &lt;- deg_props_2hrs %&gt;% filter(colony == &quot;Kent&quot;, deployment_period == &quot;2018-2019&quot;) %&gt;% select(bird_id, date, start_time, end_time, prop_wet) kent_2018_props ## # A tibble: 38,209 × 5 ## bird_id date start_time end_time prop_wet ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BP488 2018-09-12 00:00:00 01:59:59 0 ## 2 BP488 2018-09-12 02:00:00 03:59:59 0 ## 3 BP488 2018-09-12 04:00:00 05:59:59 0 ## 4 BP488 2018-09-12 06:00:00 07:59:59 0 ## 5 BP488 2018-09-12 08:00:00 09:59:59 0 ## 6 BP488 2018-09-12 10:00:00 11:59:59 0 ## 7 BP488 2018-09-12 12:00:00 13:59:59 0 ## 8 BP488 2018-09-12 14:00:00 15:59:59 0 ## 9 BP488 2018-09-12 16:00:00 17:59:59 0 ## 10 BP488 2018-09-12 18:00:00 19:59:59 0 ## # ℹ 38,199 more rows plot_kent2018_props &lt;- plot_props(data = kent_2018_props, y_var = &quot;prop_wet&quot;, y_label = &quot;Proportion&quot;, plot_title = &quot;Proportion of Wet Time Across Deployments Per 2hr bins) - (Kent Island 2018-2019)&quot;) Daily Proportions: kent_2018_daily_props &lt;- kent_2018_props %&gt;% group_by(bird_id, date) %&gt;% summarize(daily_prop_wet = mean(prop_wet, na.rm = TRUE), .groups = &quot;drop&quot;) plot_kent2018_daily_props &lt;- plot_daily_props(data = kent_2018_daily_props, y_var = &quot;daily_prop_wet&quot;, y_label = &quot;Proportion&quot;, plot_title = &quot;Mean Proportion of Wet Periods (2hr bins) Per Day (Kent Island 2018-2019)&quot;) Daily Switches: #subset the dataframe so we have less data to work with kent_2018_switches &lt;- daily_switches %&gt;% filter(colony == &quot;Kent&quot;, deployment_period == &quot;2018-2019&quot;) %&gt;% select(bird_id, date, daily_switches) head(kent_2018_switches) ## # A tibble: 6 × 3 ## bird_id date daily_switches ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 BP488 2018-09-12 0 ## 2 BP488 2018-09-13 36 ## 3 BP488 2018-09-14 25 ## 4 BP488 2018-09-15 39 ## 5 BP488 2018-09-16 36 ## 6 BP488 2018-09-17 38 plot_kent2018_daily_switches &lt;- plot_daily_switches( data = kent_2018_switches, y_var = &quot;daily_switches&quot;, y_label = &quot;Sum of Switches&quot;, plot_title = &quot;Daily Sum of Switches between Wet (wet&gt;0) and Dry (wet=0) States for Kent Island 2018-2019&quot;) Longest Wet/Dry Periods kent_2018_longest_prds &lt;- deg_longest_prds %&gt;% filter(colony == &quot;Kent&quot;, deployment_period == &quot;2018-2019&quot;) %&gt;% select(bird_id, longest_dry_time, longest_dry_start, longest_dry_end, longest_wet_time, longest_wet_start, longest_wet_end) plot_kent_2018_longest_prds &lt;- plot_longest_prds( data = kent_2018_longest_prds, plot_title = &quot;Longest Wet and Dry Periods for Kent Island 2018-2019&quot;) Total Wet/Dry kent_2018_total_wetdry &lt;- deg_total_wetdry %&gt;% filter(colony == &quot;Kent&quot;, deployment_period == &quot;2018-2019&quot;) %&gt;% select(bird_id, total_time_wet, total_time_dry) plot_kent_2018_total_times &lt;- plot_total_times(kent_2018_total_wetdry, &quot;Total Wet and Dry Times (Kent Island 2018-2019)&quot;) 8.2.2 Final Plots plot_kent2018_props plot_kent2018_daily_props plot_kent2018_daily_switches plot_kent_2018_total_times plot_kent_2018_longest_prds 8.3 Random subset Now let’s visualize the switches/proportions for 10 random files Make sure you clear out your environment so you don’t accidentally run this on Baccalieu- or Kent-related objects or other sites you may have tried out. rm(list = ls(pattern = &quot;^kent&quot;), envir = .GlobalEnv) rm(list = ls(pattern = &quot;^bacc&quot;), envir = .GlobalEnv) 8.3.1 Running plotting functions Let’s start by subsetting the metadata file so we can look at 10 files from any random year and place. random_bird_ids &lt;- metrics_md %&gt;% distinct(bird_id) %&gt;% sample_n(10) %&gt;% pull(bird_id) random_bird_ids ## [1] &quot;CD606&quot; &quot;CH713&quot; &quot;CD489&quot; &quot;BU831&quot; &quot;CH023&quot; &quot;CD474&quot; &quot;BU468&quot; &quot;CD454&quot; &quot;CH789&quot; ## [10] &quot;BU752&quot; Proportions: #subset the dataframe so we have less data to work with random_props &lt;- deg_props_2hrs %&gt;% filter(bird_id %in% random_bird_ids) %&gt;% select(bird_id, colony, date, start_time, end_time, prop_wet) #make sure it filtered the same random files properly print(unique(random_props$bird_id)) ## [1] &quot;BU468&quot; &quot;BU752&quot; &quot;BU831&quot; &quot;CD454&quot; &quot;CD474&quot; &quot;CD489&quot; &quot;CD606&quot; &quot;CH023&quot; &quot;CH713&quot; ## [10] &quot;CH789&quot; plot_random_props &lt;- plot_props(data = random_props, y_var = &quot;prop_wet&quot;, y_label = &quot;Proportion&quot;, plot_title = &quot;Proportion of Wet Time Across Deployments Per 2hr bins) - (10 random files)&quot;) Daily Proportions: random_daily_props &lt;- random_props %&gt;% group_by(bird_id, date) %&gt;% summarize(daily_prop_wet = mean(prop_wet, na.rm = TRUE), .groups = &quot;drop&quot;) plot_random_daily_props &lt;- plot_daily_props(data = random_daily_props, y_var = &quot;daily_prop_wet&quot;, y_label = &quot;Proportion&quot;, plot_title = &quot;Mean Proportion of Wet Periods (2hr bins) Per Day (10 random files)&quot;) Daily Switches: #subset the dataframe so we have less data to work with random_switches &lt;- daily_switches %&gt;% filter(bird_id %in% random_bird_ids) %&gt;% select(bird_id, date, daily_switches) #make sure it filtered the same random files print(unique(random_switches$bird_id)) ## [1] &quot;BU468&quot; &quot;BU752&quot; &quot;BU831&quot; &quot;CD454&quot; &quot;CD474&quot; &quot;CD489&quot; &quot;CD606&quot; &quot;CH023&quot; &quot;CH713&quot; ## [10] &quot;CH789&quot; plot_random_daily_switches &lt;- plot_daily_switches( data = random_switches, y_var = &quot;daily_switches&quot;, y_label = &quot;Sum of Switches&quot;, plot_title = &quot;Daily Sum of Switches between Wet (wet&gt;0) and Dry (wet=0) States for 10 Random Files&quot;) Longest Wet/Dry Periods random_longest_prds &lt;- deg_longest_prds %&gt;% filter(bird_id %in% random_bird_ids) %&gt;% select(bird_id, longest_dry_time, longest_dry_start, longest_dry_end, longest_wet_time, longest_wet_start, longest_wet_end) plot_random_longest_prds &lt;- plot_longest_prds( data = random_longest_prds, plot_title = &quot;Longest Wet and Dry Periods for 10 Random Files&quot;) Total Wet/Dry random_total_wetdry &lt;- deg_total_wetdry %&gt;% filter(bird_id %in% random_bird_ids) %&gt;% select(bird_id, total_time_wet, total_time_dry) plot_random_total_times &lt;- plot_total_times(random_total_wetdry, &quot;Total Wet and Dry Times for 10 Random Files&quot;) 8.3.2 Final Plots plot_random_props plot_random_daily_props plot_random_daily_switches plot_random_total_times plot_random_longest_prds "],["hidden-markov-modelling-hmm.html", "9 Hidden Markov Modelling (HMM) 9.1 Prep the data", " 9 Hidden Markov Modelling (HMM) 9.1 Prep the data create a single clean data frame containing data for every file Run a 2state HMMM, have to run the following for the HMM: prepData() define variables: stateNames &lt;- dry, wet stepPar &lt;- (inital mean for states based on prop_wet) anglePar &lt;- (no angle parameter for our analysis) fitHMM() plotSates() - visualize the model test with different binning intervals (2hr, 5hr?) test covariates wetness level (0-20) as a covariate. maybe seasonality or a day/night indicator? REFERENCES: https://cran.r-project.org/web/packages/momentuHMM/vignettes/momentuHMM.pdf https://mjones029.github.io/Tutorials/HMM_tutorial.html "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
